name: Comprehensive Test Suite

# Add permissions for GitHub Actions
permissions:
  contents: write
  pull-requests: write
  issues: write
  packages: write

on:
  push:
    branches:
      - 'master' # Production branch
      - 'develop' # Main development branch
      - 'dev' # Development branch
  pull_request:
    branches:
      - '*' # PRs targeting any branch
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - security

env:
  NODE_VERSION: '20'
  CACHE_KEY_PREFIX: 'discord-bot-test'
  GH_TOKEN: ${{ github.token }}

jobs:
  # Pre-flight checks
  pre-flight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.check.outputs.should-run }}
      test-matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check if tests should run
        id: check
        run: |
          set -e

          # Validate inputs for security
          if [[ "${{ github.event_name }}" == "push" ]] && [[ -n "${{ github.event.before }}" ]]; then
            if ! [[ "${{ github.event.before }}" =~ ^[a-f0-9]{7,40}$ ]]; then
              echo "Invalid before SHA format, running tests for safety"
              echo "should-run=true" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi

          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Always run tests on PRs
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "push" ]]; then
            # Run tests if relevant files changed
            if git diff --name-only "${{ github.event.before }}" "${{ github.sha }}" | grep -E '\.(js|json|yml|yaml)$' >/dev/null 2>&1; then
              echo "should-run=true" >> $GITHUB_OUTPUT
            else
              echo "should-run=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate test matrix
        id: matrix
        run: |
          if [[ "${{ github.event.inputs.test_type }}" == "unit" ]]; then
            echo 'matrix={"test-type": ["unit"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "integration" ]]; then
            echo 'matrix={"test-type": ["integration"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "e2e" ]]; then
            echo 'matrix={"test-type": ["e2e"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "performance" ]]; then
            echo 'matrix={"test-type": ["performance"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "security" ]]; then
            echo 'matrix={"test-type": ["security"]}' >> $GITHUB_OUTPUT
          else
            echo 'matrix={"test-type": ["unit", "integration", "e2e", "performance", "security"]}' >> $GITHUB_OUTPUT
          fi

  # Code quality and linting
  lint:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: Cache node modules
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
          key: ${{ runner.os }}-lint-${{ hashFiles('**/package-lock.json') }}-v2
          restore-keys: |
            ${{ runner.os }}-lint-${{ hashFiles('**/package-lock.json') }}-v2
            ${{ runner.os }}-lint-

      - name: Install dependencies
        run: |
          # Install with retry logic for corrupted tarball handling
          # Configure npm settings for better reliability
          npm config set fetch-retries 5
          npm config set fetch-retry-mintimeout 20000
          npm config set fetch-retry-maxtimeout 120000
          npm config set registry https://registry.npmjs.org/
          
          for i in {1..3}; do
            echo "Attempt $i to install dependencies..."
            if npm ci --prefer-offline --no-audit; then
              echo "Dependencies installed successfully on attempt $i"
              break
            else
              echo "Failed attempt $i"
              if [ $i -lt 3 ]; then
                echo "Clearing npm cache and retrying..."
                npm cache clean --force
                rm -rf node_modules package-lock.json
                npm cache verify
                echo "Waiting $((i * 5)) seconds before retry..."
                sleep $((i * 5))
              else
                echo "All npm install attempts failed"
                exit 1
              fi
            fi
          done

      - name: Run ESLint
        run: |
          if [ -f eslint.config.mjs ] || [ -f .eslintrc.js ] || [ -f .eslintrc.json ]; then
            npm run lint || echo "ESLint found issues but continuing with build"
          else
            echo "No ESLint configuration found, skipping lint check"
          fi

      - name: Run Prettier check
        run: |
          if [ -f .prettierrc ] || [ -f .prettierrc.json ]; then
            npx prettier --check "**/*.{js,json,md,yml,yaml}" || echo "Prettier found formatting issues but continuing with build"
          else
            echo "No Prettier configuration found, skipping format check"
          fi

      - name: Check for security vulnerabilities
        run: npm audit --audit-level=moderate

      - name: Validate package.json
        run: npm ls --depth=0

  # Unit tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'unit')
    strategy:
      matrix:
        node-version: ['18', '20']
      fail-fast: false
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: Cache Jest
        uses: actions/cache@v4
        with:
          path: .jest-cache
          key: ${{ runner.os }}-jest-node${{ matrix.node-version }}-${{ hashFiles('jest.config.js', 'package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-jest-node${{ matrix.node-version }}-

      - name: Install dependencies
        run: |
          # Install with retry logic for corrupted tarball handling
          # Configure npm settings for better reliability
          npm config set fetch-retries 5
          npm config set fetch-retry-mintimeout 20000
          npm config set fetch-retry-maxtimeout 120000
          npm config set registry https://registry.npmjs.org/
          
          for i in {1..3}; do
            echo "Attempt $i to install dependencies..."
            if npm ci --prefer-offline --no-audit; then
              echo "Dependencies installed successfully on attempt $i"
              break
            else
              echo "Failed attempt $i"
              if [ $i -lt 3 ]; then
                echo "Clearing npm cache and retrying..."
                npm cache clean --force
                rm -rf node_modules package-lock.json
                npm cache verify
                echo "Waiting $((i * 5)) seconds before retry..."
                sleep $((i * 5))
              else
                echo "All npm install attempts failed"
                exit 1
              fi
            fi
          done

      - name: Run unit tests
        run: |
          mkdir -p coverage/unit
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npm run test:unit -- \
            --config=jest.ci.config.js \
            --coverage \
            --coverageDirectory=coverage/unit \
            --coverageReporters=lcov \
            --maxWorkers=1 \
            --testTimeout=10000 \
            --forceExit \
            --detectOpenHandles \
            --verbose=true 2>&1 | tee coverage/unit/test-output-node${{ matrix.node-version }}.log

      - name: Upload unit test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/unit/lcov.info
          flags: unit-node${{ matrix.node-version }}
          name: unit-tests-node${{ matrix.node-version }}
          fail_ci_if_error: false
          plugins: ''

      - name: Store unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-node${{ matrix.node-version }}
          path: |
            coverage/unit/
            junit.xml
          retention-days: 7

# Build and Push Docker Image
  build-and-push-docker:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: [pre-flight]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'integration')
    outputs:
      image_name: ghcr.io/${{ steps.string.outputs.repo_lc }}
      image_tag: ${{ steps.final-image.outputs.image_tag_final }}
      image_digest: ${{ steps.build.outputs.digest || steps.force-build.outputs.digest }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set repo to lowercase
        id: string
        run: echo "repo_lc=$(echo '${{ github.repository }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT

      - name: Check if Docker build is needed
        id: docker-changes
        run: |
          # For PRs and first push, always build
          if [[ "${{ github.event_name }}" == "pull_request" ]] || [[ "${{ github.event.before }}" == "0000000000000000000000000000000000000000" ]]; then
            echo "docker_changed=true" >> $GITHUB_OUTPUT
            echo "Building Docker image for PR or initial push"
            exit 0
          fi

          # Check if Docker-related files changed since last commit
          if git diff --name-only "${{ github.event.before }}" "${{ github.sha }}" | grep -E '^(Dockerfile|\.dockerignore|package\.json|package-lock\.json|src/|tests/)$'; then
            echo "docker_changed=true" >> $GITHUB_OUTPUT
            echo "Docker-related files changed, will build new image"
          else
            echo "docker_changed=false" >> $GITHUB_OUTPUT
            echo "No Docker-related changes detected, will try to use cached image"
          fi

      - name: Generate Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ steps.string.outputs.repo_lc }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-,enable={{is_default_branch}}
            type=sha
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.title=Discord YouTube Bot
            org.opencontainers.image.description=Integration test environment
            org.opencontainers.image.vendor=discord-youtube-bot

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver-opts: |
            network=host

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        id: build
        if: steps.docker-changes.outputs.docker_changed == 'true'
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile
          platforms: linux/amd64
          target: test-runner
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILDKIT_INLINE_CACHE=1

      - name: Use existing image if no changes
        id: existing-image
        if: steps.docker-changes.outputs.docker_changed == 'false'
        run: |
          # Try to pull the latest image to use as fallback
          LATEST_TAG="ghcr.io/${{ steps.string.outputs.repo_lc }}:${{ github.ref_name }}-${{ github.sha }}"
          FALLBACK_TAG="ghcr.io/${{ steps.string.outputs.repo_lc }}:${{ github.ref_name }}"

          # Check if the exact SHA-based image exists
          if docker manifest inspect "$LATEST_TAG" >/dev/null 2>&1; then
            echo "Using existing image: $LATEST_TAG"
            echo "image_tag_raw=$LATEST_TAG" >> $GITHUB_OUTPUT
          elif docker manifest inspect "$FALLBACK_TAG" >/dev/null 2>&1; then
            echo "Using fallback image: $FALLBACK_TAG"
            echo "image_tag_raw=$FALLBACK_TAG" >> $GITHUB_OUTPUT
          else
            echo "No existing image found, forcing rebuild"
            echo "force_rebuild=true" >> $GITHUB_OUTPUT
          fi

      - name: Force rebuild if no cached image available
        id: force-build
        if: steps.docker-changes.outputs.docker_changed == 'false' && steps.existing-image.outputs.force_rebuild == 'true'
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile
          platforms: linux/amd64
          target: test-runner
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILDKIT_INLINE_CACHE=1

      - name: Set final image tag
        id: final-image
        run: |
          FINAL_TAG=""
          if [[ "${{ steps.docker-changes.outputs.docker_changed }}" == "true" ]] || [[ "${{ steps.existing-image.outputs.force_rebuild }}" == "true" ]]; then
            # A new image was built (or force-rebuilt). Use the first tag from the metadata action.
            FINAL_TAG=$(echo "${{ steps.meta.outputs.tags }}" | head -n 1)
          else
            # Use the tag for the existing image.
            FINAL_TAG="${{ steps.existing-image.outputs.image_tag_raw }}"
          fi
          echo "image_tag_final=${FINAL_TAG}" >> $GITHUB_OUTPUT

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    #container:
    #  image: ${{ needs.build-and-push-docker.outputs.image_tag }}
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    #needs: [pre-flight, lint, build-and-push-docker]
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'integration')
    services:
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: Install dependencies
        run: |
          # Install with retry logic for corrupted tarball handling
          # Configure npm settings for better reliability
          npm config set fetch-retries 5
          npm config set fetch-retry-mintimeout 20000
          npm config set fetch-retry-maxtimeout 120000
          npm config set registry https://registry.npmjs.org/
          
          for i in {1..3}; do
            echo "Attempt $i to install dependencies..."
            if npm ci --prefer-offline --no-audit; then
              echo "Dependencies installed successfully on attempt $i"
              break
            else
              echo "Failed attempt $i"
              if [ $i -lt 3 ]; then
                echo "Clearing npm cache and retrying..."
                npm cache clean --force
                rm -rf node_modules package-lock.json
                npm cache verify
                echo "Waiting $((i * 5)) seconds before retry..."
                sleep $((i * 5))
              else
                echo "All npm install attempts failed"
                exit 1
              fi
            fi
          done

      - name: Set up test environment
        run: |
          cat > .env.test << 'EOF'
          NODE_ENV=test
          DISCORD_BOT_TOKEN=test-token-placeholder
          YOUTUBE_API_KEY=test-api-key-placeholder
          YOUTUBE_CHANNEL_ID=UCtest123456789012345678
          DISCORD_YOUTUBE_CHANNEL_ID=123456789012345678
          PSH_CALLBACK_URL=https://test.example.com/webhook
          X_USER_HANDLE=testuser
          DISCORD_X_POSTS_CHANNEL_ID=234567890123456789
          DISCORD_X_REPLIES_CHANNEL_ID=345678901234567890
          DISCORD_X_QUOTES_CHANNEL_ID=456789012345678901
          DISCORD_X_RETWEETS_CHANNEL_ID=567890123456789012
          TWITTER_USERNAME=testuser
          TWITTER_PASSWORD=testpassword
          DISCORD_BOT_SUPPORT_LOG_CHANNEL=678901234567890123
          PSH_SECRET=test-webhook-secret-for-integration-testing
          REDIS_URL=redis://localhost:6379
          EOF

      - name: Run integration tests
        timeout-minutes: 10
        env:
          NODE_ENV: test
        run: |
          mkdir -p coverage/integration
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npm run test:integration -- \
            --config=jest.ci.config.js \
            --coverage \
            --coverageDirectory=coverage/integration \
            --coverageReporters=lcov \
            --testTimeout=15000 \
            --forceExit \
            --detectOpenHandles \
            --verbose=true 2>&1 | tee coverage/integration/test-output.log

      - name: Upload integration test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/integration/lcov.info
          flags: integration
          name: integration-tests
          fail_ci_if_error: false
          plugins: ''

      - name: Store integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            coverage/integration/
            logs/
          retention-days: 7

  # End-to-end tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'e2e')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: Install dependencies
        run: |
          # Install with retry logic for corrupted tarball handling
          # Configure npm settings for better reliability
          npm config set fetch-retries 5
          npm config set fetch-retry-mintimeout 20000
          npm config set fetch-retry-maxtimeout 120000
          npm config set registry https://registry.npmjs.org/
          
          for i in {1..3}; do
            echo "Attempt $i to install dependencies..."
            if npm ci --prefer-offline --no-audit; then
              echo "Dependencies installed successfully on attempt $i"
              break
            else
              echo "Failed attempt $i"
              if [ $i -lt 3 ]; then
                echo "Clearing npm cache and retrying..."
                npm cache clean --force
                rm -rf node_modules package-lock.json
                npm cache verify
                echo "Waiting $((i * 5)) seconds before retry..."
                sleep $((i * 5))
              else
                echo "All npm install attempts failed"
                exit 1
              fi
            fi
          done

      - name: Set up E2E test environment
        run: |
          cat > .env.e2e << 'EOF'
          NODE_ENV=test
          LOG_LEVEL=error
          DISCORD_BOT_TOKEN=test-bot-token-e2e-placeholder
          YOUTUBE_API_KEY=test-youtube-api-key-placeholder
          YOUTUBE_CHANNEL_ID=UCtest123456789012345678
          DISCORD_YOUTUBE_CHANNEL_ID=123456789012345678
          PSH_CALLBACK_URL=https://test-e2e.example.com/webhook
          PSH_PORT=3001
          X_USER_HANDLE=testuser
          DISCORD_X_POSTS_CHANNEL_ID=234567890123456789
          DISCORD_X_REPLIES_CHANNEL_ID=345678901234567890
          DISCORD_X_QUOTES_CHANNEL_ID=456789012345678901
          DISCORD_X_RETWEETS_CHANNEL_ID=567890123456789012
          TWITTER_USERNAME=testuser
          TWITTER_PASSWORD=testpassword
          DISCORD_BOT_SUPPORT_LOG_CHANNEL=678901234567890123
          PSH_SECRET=e2e-test-webhook-secret
          ANNOUNCEMENT_ENABLED=true
          X_VX_TWITTER_CONVERSION=false
          ALLOWED_USER_IDS=123456789012345678,987654321098765432
          EOF

      - name: Run E2E tests
        env:
          NODE_ENV: test
        run: |
          mkdir -p coverage/e2e
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npm run test:e2e -- \
            --coverage \
            --coverageDirectory=coverage/e2e \
            --coverageReporters=lcov \
            --testTimeout=30000 \
            --forceExit \
            --detectOpenHandles \
            --verbose=true

      - name: Upload unit test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/e2e/lcov.info
          flags: e2e-node${{ matrix.node-version }}
          name: e2e-tests-node${{ matrix.node-version }}
          fail_ci_if_error: false
          plugins: ''


      - name: Store E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            coverage/e2e/
            logs/
            screenshots/
            .env.e2e
            test-results/e2e/
          retention-days: 7
          if-no-files-found: warn

  # Performance tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'performance')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: Install dependencies
        run: |
          # Install with retry logic for corrupted tarball handling
          # Configure npm settings for better reliability
          npm config set fetch-retries 5
          npm config set fetch-retry-mintimeout 20000
          npm config set fetch-retry-maxtimeout 120000
          npm config set registry https://registry.npmjs.org/
          
          for i in {1..3}; do
            echo "Attempt $i to install dependencies..."
            if npm ci --prefer-offline --no-audit; then
              echo "Dependencies installed successfully on attempt $i"
              break
            else
              echo "Failed attempt $i"
              if [ $i -lt 3 ]; then
                echo "Clearing npm cache and retrying..."
                npm cache clean --force
                rm -rf node_modules package-lock.json
                npm cache verify
                echo "Waiting $((i * 5)) seconds before retry..."
                sleep $((i * 5))
              else
                echo "All npm install attempts failed"
                exit 1
              fi
            fi
          done

      - name: Run performance tests
        run: |
          mkdir -p coverage/performance
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=6144" \
          npm run test:performance -- \
            --coverage \
            --coverageDirectory=coverage/performance \
            --coverageReporters=lcov \
            --maxWorkers=1 \
            --testTimeout=60000 \
            --forceExit \
            --detectOpenHandles \
            --verbose=true

      - name: Generate performance report
        run: |
          mkdir -p reports
          echo "# Performance Test Report" > reports/performance.md
          echo "Generated on: $(date)" >> reports/performance.md
          echo "Node.js version: $(node --version)" >> reports/performance.md
          echo "Platform: ${{ runner.os }}" >> reports/performance.md
          echo "" >> reports/performance.md

          # Extract performance metrics from test output
          echo "## Performance Metrics" >> reports/performance.md
          echo "" >> reports/performance.md

          # Parse Jest output for timing information
          if [ -f coverage/performance/test-output.log ]; then
            echo "### Test Execution Times" >> reports/performance.md
            grep -E "Test completed in [0-9]+\.[0-9]+ms" coverage/performance/test-output.log | head -10 >> reports/performance.md || echo "No timing data found" >> reports/performance.md
            echo "" >> reports/performance.md
            
            echo "### Throughput Metrics" >> reports/performance.md
            grep -E "(messages per second|URLs per second|webhooks per second)" coverage/performance/test-output.log | head -10 >> reports/performance.md || echo "No throughput data found" >> reports/performance.md
            echo "" >> reports/performance.md
          fi

          # Add system performance info
          echo "### System Performance" >> reports/performance.md
          echo "- **CPU Info**: $(nproc) cores" >> reports/performance.md
          echo "- **Memory**: $(free -h | grep '^Mem:' | awk '{print $2}')" >> reports/performance.md
          echo "- **Test Duration**: $(date -d @$SECONDS -u '+%M:%S')" >> reports/performance.md
          echo "" >> reports/performance.md

          # Create metrics summary JSON
          cat > performance-metrics.json << EOF
          {
            "timestamp": "$(date -Iseconds)",
            "node_version": "$(node --version)",
            "platform": "${{ runner.os }}",
            "cpu_cores": $(nproc),
            "memory_total": "$(free -h | grep '^Mem:' | awk '{print $2}')",
            "test_duration_seconds": $SECONDS
          }
          EOF

      - name: Upload performance test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/performance/lcov.info
          flags: performance
          name: performance-tests
          fail_ci_if_error: false
          plugins: ''

      - name: Store performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            coverage/performance/
            reports/
            performance-metrics.json
          retention-days: 30

  # Security tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'security')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: Install dependencies
        run: |
          # Install with retry logic for corrupted tarball handling
          # Configure npm settings for better reliability
          npm config set fetch-retries 5
          npm config set fetch-retry-mintimeout 20000
          npm config set fetch-retry-maxtimeout 120000
          npm config set registry https://registry.npmjs.org/
          
          for i in {1..3}; do
            echo "Attempt $i to install dependencies..."
            if npm ci --prefer-offline --no-audit; then
              echo "Dependencies installed successfully on attempt $i"
              break
            else
              echo "Failed attempt $i"
              if [ $i -lt 3 ]; then
                echo "Clearing npm cache and retrying..."
                npm cache clean --force
                rm -rf node_modules package-lock.json
                npm cache verify
                echo "Waiting $((i * 5)) seconds before retry..."
                sleep $((i * 5))
              else
                echo "All npm install attempts failed"
                exit 1
              fi
            fi
          done

      - name: Run security tests
        run: |
          mkdir -p test-results/security
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npx jest --config tests/configs/jest.security.config.js \
            --maxWorkers=1 \
            --testTimeout=20000 \
            --forceExit \
            --detectOpenHandles 

      - name: Run security audit
        run: |
          npm audit --audit-level=moderate --json > security-audit.json || true

          # Check for high/critical vulnerabilities
          HIGH_VULNS=$(cat security-audit.json | jq '.metadata.vulnerabilities.high // 0')
          CRITICAL_VULNS=$(cat security-audit.json | jq '.metadata.vulnerabilities.critical // 0')

          echo "High vulnerabilities: $HIGH_VULNS"
          echo "Critical vulnerabilities: $CRITICAL_VULNS"

          if [ "$CRITICAL_VULNS" -gt 0 ]; then
            echo "❌ Critical vulnerabilities found!"
            exit 1
          elif [ "$HIGH_VULNS" -gt 0 ]; then
            echo "⚠️ High vulnerabilities found!"
          fi

      - name: Check for secrets
        run: |
          # Simple check for potential secrets in code
          echo "Checking for potential secrets..."

          # Check for potential API keys, tokens, etc.
          SECRET_PATTERNS=(
            "api[_-]?key"
            "secret[_-]?key"
            "access[_-]?token"
            "auth[_-]?token"
            "discord[_-]?token"
            "password"
          )

          for pattern in "${SECRET_PATTERNS[@]}"; do
            if git grep -i "$pattern" -- '*.js' '*.json' | grep -v test | grep -v mock | grep -v example; then
              echo "⚠️ Potential secret found: $pattern"
            fi
          done

      - name: Store security test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-test-results
          path: |
            security-audit.json
            security-report.json
            test-results/security/
          retention-days: 30

  # Aggregate results and report
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    timeout-minutes: 5
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs:
      [
        unit-tests,
        integration-tests,
        e2e-tests,
        performance-tests,
        security-tests,
      ]
    if: always() && needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: Install dependencies
        run: |
          # Install with retry logic for corrupted tarball handling
          # Configure npm settings for better reliability
          npm config set fetch-retries 5
          npm config set fetch-retry-mintimeout 20000
          npm config set fetch-retry-maxtimeout 120000
          npm config set registry https://registry.npmjs.org/
          
          for i in {1..3}; do
            echo "Attempt $i to install dependencies..."
            if npm ci --prefer-offline --no-audit; then
              echo "Dependencies installed successfully on attempt $i"
              break
            else
              echo "Failed attempt $i"
              if [ $i -lt 3 ]; then
                echo "Clearing npm cache and retrying..."
                npm cache clean --force
                rm -rf node_modules package-lock.json
                npm cache verify
                echo "Waiting $((i * 5)) seconds before retry..."
                sleep $((i * 5))
              else
                echo "All npm install attempts failed"
                exit 1
              fi
            fi
          done

      - name: Install coverage tools
        run: |
          # Install coverage tools with retry logic
          # Ensure lcov is installed for genhtml
          sudo apt-get update && sudo apt-get install -y lcov

          # Install coverage tools with retry logic
          for i in {1..3}; do
            echo "Attempt $i to install coverage tools..."
            if npm install -g nyc; then
              echo "Coverage tools installed successfully on attempt $i"
              break
            else
              echo "Failed attempt $i"
              if [ $i -lt 3 ]; then
                echo "Clearing npm cache and retrying..."
                npm cache clean --force
                sleep $((i * 3))
              else
                echo "Coverage tools installation failed after 3 attempts"
                exit 1
              fi
            fi
          done

      - name: Download Node 18 unit test coverage
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: unit-test-results-node18
          path: test-results/unit/node18/

      - name: Download Node 20 unit test coverage
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: unit-test-results-node20
          path: test-results/unit/node20/

      - name: Download test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          pattern: '*-test-results'
          merge-multiple: true
          path: test-results/

      - name: Merge coverage reports (simplified)
        id: coverage
        run: |
          mkdir -p coverage/merged

          echo "Collecting coverage files..."

          # Find primary coverage file (Node 18 unit tests)
          PRIMARY_COVERAGE=""
          if [ -f "test-results/unit/node18/coverage/unit/lcov.info" ]; then
            PRIMARY_COVERAGE="test-results/unit/node18/coverage/unit/lcov.info"
            echo "Found primary unit test coverage"
          elif [ -f "test-results/unit/node20/coverage/unit/lcov.info" ]; then
            PRIMARY_COVERAGE="test-results/unit/node20/coverage/unit/lcov.info"
            echo "Using Node 20 coverage as primary"
          fi

          if [ -n "$PRIMARY_COVERAGE" ] && [ -s "$PRIMARY_COVERAGE" ]; then
            # Use primary coverage as baseline
            cp "$PRIMARY_COVERAGE" coverage/merged/lcov.info
            
            echo "Using primary coverage file with $(wc -l < coverage/merged/lcov.info) lines"
            
            # Extract coverage metrics
            LINES_FOUND=$(grep -E "^LF:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
            LINES_HIT=$(grep -E "^LH:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
            FUNCTIONS_FOUND=$(grep -E "^FNF:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
            FUNCTIONS_HIT=$(grep -E "^FNH:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
            BRANCHES_FOUND=$(grep -E "^BRF:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
            BRANCHES_HIT=$(grep -E "^BRH:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
            
            # Calculate percentages
            LINES_PCT=$(awk -v hit="$LINES_HIT" -v found="$LINES_FOUND" 'BEGIN {
              if (found == 0) print "0.00"; else printf "%.2f", hit * 100 / found
            }')
            FUNCTIONS_PCT=$(awk -v hit="$FUNCTIONS_HIT" -v found="$FUNCTIONS_FOUND" 'BEGIN {
              if (found == 0) print "0.00"; else printf "%.2f", hit * 100 / found
            }')
            BRANCHES_PCT=$(awk -v hit="$BRANCHES_HIT" -v found="$BRANCHES_FOUND" 'BEGIN {
              if (found == 0) print "0.00"; else printf "%.2f", hit * 100 / found
            }')
            
            echo "Coverage: Lines ${LINES_PCT}%, Functions ${FUNCTIONS_PCT}%, Branches ${BRANCHES_PCT}%"
            
            # Generate simplified coverage summary
            cat > coverage-summary.json << EOF
          {
            "total": {
              "lines": {
                "total": ${LINES_FOUND},
                "covered": ${LINES_HIT},
                "pct": ${LINES_PCT}
              },
              "statements": {
                "total": ${LINES_FOUND},
                "covered": ${LINES_HIT},
                "pct": ${LINES_PCT}
              },
              "functions": {
                "total": ${FUNCTIONS_FOUND},
                "covered": ${FUNCTIONS_HIT},
                "pct": ${FUNCTIONS_PCT}
              },
              "branches": {
                "total": ${BRANCHES_FOUND},
                "covered": ${BRANCHES_HIT},
                "pct": ${BRANCHES_PCT}
              }
            }
          }
          EOF
            
            echo "coverage_pct=${LINES_PCT}" >> $GITHUB_OUTPUT
            echo "status=available" >> $GITHUB_OUTPUT
          else
            echo "No coverage files found"
            echo '{"total":{"lines":{"pct":0},"functions":{"pct":0},"branches":{"pct":0}}}' > coverage-summary.json
            echo "coverage_pct=0" >> $GITHUB_OUTPUT
            echo "status=missing" >> $GITHUB_OUTPUT
          fi

      - name: Generate LCOV HTML Report
        if: steps.coverage.outputs.status == 'available'
        run: |
          set -e # Exit immediately if a command exits with a non-zero status

          LCOV_REPORT_DIR="lcov-html-report"
          mkdir -p "$LCOV_REPORT_DIR" # Quoting for safety

          echo "Current working directory: $(pwd)"
          echo "Listing contents of assets/lcov/ for debugging:"
          ls -lR ./assets/lcov/ # Output of this was already good, confirms file presence

          # Define full source and destination paths for clarity and debugging
          SOURCE_BASE_CSS="$(pwd)/assets/lcov/base.css"
          SOURCE_PRETTIFY_CSS="$(pwd)/assets/lcov/prettify.css"
          SOURCE_DARK_THEME_CSS="$(pwd)/assets/lcov/lcov-dark-theme.css"

          DEST_BASE_CSS="$LCOV_REPORT_DIR/base.css"
          DEST_PRETTIFY_CSS="$LCOV_REPORT_DIR/prettify.css"
          DEST_DARK_THEME_CSS="$LCOV_REPORT_DIR/lcov-dark-theme.css"

          # Debugging: Print the exact paths before copying
          echo "Attempting to copy base.css from: $SOURCE_BASE_CSS"
          echo "Attempting to copy prettify.css from: $SOURCE_PRETTIFY_CSS"
          echo "Attempting to symlink lcov-dark-theme.css from: $SOURCE_DARK_THEME_CSS"
          echo "Attempting to copy to destination directory: $LCOV_REPORT_DIR"
          # Use verbose copy (-v) for more output
          cp -v "$SOURCE_BASE_CSS" "$DEST_BASE_CSS"
          cp -v "$SOURCE_PRETTIFY_CSS" "$DEST_PRETTIFY_CSS"

          # Create a symbolic link in the current working directory for genhtml to find
          echo "Creating temporary symlink for lcov-dark-theme.css in current directory."
          ln -s "$SOURCE_DARK_THEME_CSS" "lcov-dark-theme.css"

          echo "All CSS files copied successfully."

          genhtml coverage/merged/lcov.info \
            --output-directory "$LCOV_REPORT_DIR" \
            --css-file lcov-dark-theme.css \
            --title "Code Coverage Report" \
            --branch-coverage \
            --rc lcov_html_css_file="lcov-dark-theme.css" \
            --prefix "$(pwd)" # Important for correct source paths
          echo "LCOV HTML report generated in $LCOV_REPORT_DIR"


      - name: Generate comprehensive test report
        env:
          UNIT_TESTS_RESULT: ${{ needs.unit-tests.result }}
          INTEGRATION_TESTS_RESULT: ${{ needs.integration-tests.result }}
          E2E_TESTS_RESULT: ${{ needs.e2e-tests.result }}
          PERFORMANCE_TESTS_RESULT: ${{ needs.performance-tests.result }}
          SECURITY_TESTS_RESULT: ${{ needs.security-tests.result }}
        run: |
          node scripts/generate-test-report.js

      - name: Validate coverage quality
        run: |
          if [ -f coverage-metrics.json ]; then
            COVERAGE=$(jq -r '.coverage.lines // 0' coverage-metrics.json)
            echo "Current coverage: ${COVERAGE}%"
            
            # Set coverage quality gates
            if (( $(echo "$COVERAGE < 25" | bc -l) )); then
              echo "❌ Coverage critically low: $COVERAGE%"
              echo "coverage_status=critical" >> $GITHUB_ENV
            elif (( $(echo "$COVERAGE < 50" | bc -l) )); then
              echo "⚠️ Coverage below minimum: $COVERAGE%"
              echo "coverage_status=warning" >> $GITHUB_ENV
            else
              echo "✅ Coverage meets target: $COVERAGE%"
              echo "coverage_status=good" >> $GITHUB_ENV
            fi
          else
            echo "No coverage data available"
            echo "coverage_status=missing" >> $GITHUB_ENV
          fi

      - name: Upload merged coverage to Codecov
        if: steps.coverage.outputs.status == 'available'
        uses: codecov/codecov-action@v4
        with:
          file: coverage/merged/lcov.info
          flags: merged
          name: merged-coverage
          fail_ci_if_error: false
          plugins: ''

      - name: Store final test report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report-${{ github.run_id }}
          path: |
            reports/
            coverage/merged/
            coverage-summary.json
            coverage-metrics.json
            test-results/
          retention-days: 14

      - name: Upload LCOV HTML Report Artifact
        if: always() # Always upload even if genhtml failed, for debugging
        uses: actions/upload-artifact@v4
        with:
          name: lcov-html-report
          path: lcov-html-report/
          retention-days: 14 # Adjust retention as needed (e.g., 30 days)

      - name: Comment commit with test results
        if: github.event_name == 'push'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            const path = 'reports/test-summary.md';

            if (fs.existsSync(path)) {
              try {
                const body = fs.readFileSync(path, 'utf8');
                
                await github.rest.repos.createCommitComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  commit_sha: context.sha,
                  body: body
                });
                console.log('Commit comment created successfully');
              } catch (error) {
                console.log('Failed to create commit comment:', error.message);
              }
            } else {
              console.log('Test summary file not found');
            }

      - name: Store final test report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: |
            reports/
            coverage/merged/
            coverage-summary.json
            coverage-metrics.json
            test-results/
          retention-days: 30

      - name: Update status check
        if: always()
        run: |
          # Check individual job statuses and fail if any critical test failed
          UNIT_STATUS="${{ needs.unit-tests.result }}"
          INTEGRATION_STATUS="${{ needs.integration-tests.result }}"
          E2E_STATUS="${{ needs.e2e-tests.result }}"
          PERFORMANCE_STATUS="${{ needs.performance-tests.result }}"
          SECURITY_STATUS="${{ needs.security-tests.result }}"

          echo "Unit tests: $UNIT_STATUS"
          echo "Integration tests: $INTEGRATION_STATUS" 
          echo "E2E tests: $E2E_STATUS"
          echo "Performance tests: $PERFORMANCE_STATUS"
          echo "Security tests: $SECURITY_STATUS"

          # Fail if any test job failed (not skipped)
          if [[ "$UNIT_STATUS" == "failure" || "$INTEGRATION_STATUS" == "failure" || "$E2E_STATUS" == "failure" || "$PERFORMANCE_STATUS" == "failure" || "$SECURITY_STATUS" == "failure" ]]; then
            echo "❌ One or more test suites failed"
            exit 1
          elif [[ "$UNIT_STATUS" == "success" || "$INTEGRATION_STATUS" == "success" || "$E2E_STATUS" == "success" || "$PERFORMANCE_STATUS" == "success" || "$SECURITY_STATUS" == "success" ]]; then
            echo "✅ All executed tests completed successfully"
          else
            echo "⚠️ All tests were skipped"
          fi

  # Cleanup job to remove old artifacts
  cleanup:
    name: Cleanup Old Artifacts
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
      - name: Delete old artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const cutoff = new Date();
            cutoff.setDate(cutoff.getDate() - 30); // Keep artifacts for 30 days

            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
            });

            for (const artifact of artifacts.data.artifacts) {
              if (new Date(artifact.created_at) < cutoff) {
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id,
                });
                console.log(`Deleted artifact: ${artifact.name}`);
              }
            }
