name: Comprehensive Test Suite

# Add permissions for GitHub Actions
permissions:
  contents: read
  pull-requests: write
  issues: write
  checks: write
  actions: read

on:
  push:
    branches: [ main, master, dev, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - security

env:
  NODE_VERSION: '20'
  CACHE_KEY_PREFIX: 'discord-bot-test'

jobs:
  # Pre-flight checks
  pre-flight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.check.outputs.should-run }}
      test-matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check if tests should run
        id: check
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          else
            # Check if relevant files changed
            git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep -E '\.(js|json|yml|yaml)$' && echo "should-run=true" >> $GITHUB_OUTPUT || echo "should-run=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate test matrix
        id: matrix
        run: |
          if [[ "${{ github.event.inputs.test_type }}" == "unit" ]]; then
            echo 'matrix={"test-type": ["unit"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "integration" ]]; then
            echo 'matrix={"test-type": ["integration"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "e2e" ]]; then
            echo 'matrix={"test-type": ["e2e"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "performance" ]]; then
            echo 'matrix={"test-type": ["performance"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "security" ]]; then
            echo 'matrix={"test-type": ["security"]}' >> $GITHUB_OUTPUT
          else
            echo 'matrix={"test-type": ["unit", "integration", "e2e", "performance", "security"]}' >> $GITHUB_OUTPUT
          fi

  # Code quality and linting
  lint:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run ESLint
        run: |
          if [ -f .eslintrc.js ] || [ -f .eslintrc.json ]; then
            npm run lint || npx eslint . --ext .js --format=compact
          else
            echo "No ESLint configuration found, skipping lint check"
          fi

      - name: Run Prettier check
        run: |
          if [ -f .prettierrc ] || [ -f .prettierrc.json ]; then
            npx prettier --check "**/*.{js,json,md,yml,yaml}"
          else
            echo "No Prettier configuration found, skipping format check"
          fi

      - name: Check for security vulnerabilities
        run: npm audit --audit-level=moderate

      - name: Validate package.json
        run: npm ls --depth=0

  # Unit tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'unit')
    strategy:
      matrix:
        node-version: ['18', '20']
      fail-fast: false
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: |
          mkdir -p coverage/unit
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npm run test:unit -- \
            --coverage \
            --coverageDirectory=coverage/unit \
            --coverageReporters=text --coverageReporters=lcov --coverageReporters=clover \
            --maxWorkers=2 \
            --forceExit \
            --detectOpenHandles 2>&1 | tee coverage/unit/test-output-node${{ matrix.node-version }}.log

      - name: Upload unit test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/unit/lcov.info
          flags: unit-node${{ matrix.node-version }}
          name: unit-tests-node${{ matrix.node-version }}
          fail_ci_if_error: false
          plugins: ''

      - name: Store unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-node${{ matrix.node-version }}
          path: |
            coverage/unit/
            junit.xml
          retention-days: 7

  # Integration tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'integration')
    services:
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Set up test environment
        run: |
          # Create test configuration
          cat > .env.test << EOF
          NODE_ENV=test
          DISCORD_BOT_TOKEN=test-token-${{ github.run_id }}
          YOUTUBE_API_KEY=test-api-key
          YOUTUBE_CHANNEL_ID=UCtest123456789012345678
          DISCORD_YOUTUBE_CHANNEL_ID=123456789012345678
          PSH_CALLBACK_URL=https://test.example.com/webhook
          X_USER_HANDLE=testuser
          DISCORD_X_POSTS_CHANNEL_ID=234567890123456789
          DISCORD_X_REPLIES_CHANNEL_ID=345678901234567890
          DISCORD_X_QUOTES_CHANNEL_ID=456789012345678901
          DISCORD_X_RETWEETS_CHANNEL_ID=567890123456789012
          TWITTER_USERNAME=testuser
          TWITTER_PASSWORD=testpassword
          DISCORD_BOT_SUPPORT_LOG_CHANNEL=678901234567890123
          PSH_SECRET=test-webhook-secret-for-integration-testing-very-long-key
          REDIS_URL=redis://localhost:6379
          EOF

      - name: Run integration tests
        env:
          NODE_ENV: test
        run: |
          mkdir -p coverage/integration
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npm run test:integration -- \
            --coverage \
            --coverageDirectory=coverage/integration \
            --coverageReporters=text --coverageReporters=lcov \
            --maxWorkers=1 \
            --forceExit \
            --detectOpenHandles \
            --testTimeout=30000 2>&1 | tee coverage/integration/test-output.log

      - name: Upload integration test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/integration/lcov.info
          flags: integration
          name: integration-tests
          fail_ci_if_error: false
          plugins: ''

      - name: Store integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            coverage/integration/
            logs/
          retention-days: 7

  # End-to-end tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'e2e')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Set up E2E test environment
        run: |
          # Create comprehensive test environment
          cat > .env.e2e << EOF
          NODE_ENV=test
          LOG_LEVEL=error
          DISCORD_BOT_TOKEN=test-bot-token-e2e
          YOUTUBE_API_KEY=test-youtube-api-key
          YOUTUBE_CHANNEL_ID=UCtest123456789012345678
          DISCORD_YOUTUBE_CHANNEL_ID=123456789012345678
          PSH_CALLBACK_URL=https://test-e2e.example.com/webhook
          PSH_PORT=3001
          X_USER_HANDLE=testuser
          DISCORD_X_POSTS_CHANNEL_ID=234567890123456789
          DISCORD_X_REPLIES_CHANNEL_ID=345678901234567890
          DISCORD_X_QUOTES_CHANNEL_ID=456789012345678901
          DISCORD_X_RETWEETS_CHANNEL_ID=567890123456789012
          TWITTER_USERNAME=testuser
          TWITTER_PASSWORD=testpassword
          DISCORD_BOT_SUPPORT_LOG_CHANNEL=678901234567890123
          PSH_SECRET=e2e-test-webhook-secret-for-comprehensive-end-to-end-testing
          ANNOUNCEMENT_ENABLED=true
          X_VX_TWITTER_CONVERSION=false
          ALLOWED_USER_IDS=123456789012345678,987654321098765432
          EOF

      - name: Run E2E tests
        env:
          NODE_ENV: test
        run: |
          mkdir -p test-results/e2e
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npx jest --config jest.e2e.config.js \
            --maxWorkers=1 \
            --forceExit \
            --detectOpenHandles \
            --testTimeout=60000 2>&1 | tee test-results/e2e/test-output.log

      # E2E tests don't generate source code coverage since they focus on behavior testing

      - name: Store E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            logs/
            screenshots/
            .env.e2e
            test-results/e2e/
          retention-days: 7
          if-no-files-found: warn

  # Performance tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'performance')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run performance tests
        run: |
          mkdir -p coverage/performance
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=8192" \
          npm run test:performance -- \
            --coverage \
            --coverageDirectory=coverage/performance \
            --coverageReporters=text --coverageReporters=lcov \
            --maxWorkers=1 \
            --forceExit \
            --detectOpenHandles \
            --testTimeout=120000 \
            --verbose 2>&1 | tee coverage/performance/test-output.log

      - name: Generate performance report
        run: |
          mkdir -p reports
          echo "# Performance Test Report" > reports/performance.md
          echo "Generated on: $(date)" >> reports/performance.md
          echo "Node.js version: $(node --version)" >> reports/performance.md
          echo "Platform: ${{ runner.os }}" >> reports/performance.md
          echo "" >> reports/performance.md
          
          # Extract performance metrics from test output
          echo "## Performance Metrics" >> reports/performance.md
          echo "" >> reports/performance.md
          
          # Parse Jest output for timing information
          if [ -f coverage/performance/test-output.log ]; then
            echo "### Test Execution Times" >> reports/performance.md
            grep -E "Test completed in [0-9]+\.[0-9]+ms" coverage/performance/test-output.log | head -10 >> reports/performance.md || echo "No timing data found" >> reports/performance.md
            echo "" >> reports/performance.md
            
            echo "### Throughput Metrics" >> reports/performance.md
            grep -E "(messages per second|URLs per second|webhooks per second)" coverage/performance/test-output.log | head -10 >> reports/performance.md || echo "No throughput data found" >> reports/performance.md
            echo "" >> reports/performance.md
          fi
          
          # Add system performance info
          echo "### System Performance" >> reports/performance.md
          echo "- **CPU Info**: $(nproc) cores" >> reports/performance.md
          echo "- **Memory**: $(free -h | grep '^Mem:' | awk '{print $2}')" >> reports/performance.md
          echo "- **Test Duration**: $(date -d @$SECONDS -u '+%M:%S')" >> reports/performance.md
          echo "" >> reports/performance.md
          
          # Create metrics summary JSON
          cat > performance-metrics.json << EOF
          {
            "timestamp": "$(date -Iseconds)",
            "node_version": "$(node --version)",
            "platform": "${{ runner.os }}",
            "cpu_cores": $(nproc),
            "memory_total": "$(free -h | grep '^Mem:' | awk '{print $2}')",
            "test_duration_seconds": $SECONDS
          }
          EOF

      - name: Upload performance test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/performance/lcov.info
          flags: performance
          name: performance-tests
          fail_ci_if_error: false
          plugins: ''

      - name: Store performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            coverage/performance/
            reports/
            performance-metrics.json
          retention-days: 30

  # Security tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'security')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run security tests
        run: |
          mkdir -p test-results/security
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npx jest --config jest.security.config.js \
            --maxWorkers=2 \
            --forceExit \
            --detectOpenHandles \
            --testTimeout=45000 2>&1 | tee test-results/security/test-output.log

      - name: Run security audit
        run: |
          npm audit --audit-level=moderate --json > security-audit.json || true
          
          # Check for high/critical vulnerabilities
          HIGH_VULNS=$(cat security-audit.json | jq '.metadata.vulnerabilities.high // 0')
          CRITICAL_VULNS=$(cat security-audit.json | jq '.metadata.vulnerabilities.critical // 0')
          
          echo "High vulnerabilities: $HIGH_VULNS"
          echo "Critical vulnerabilities: $CRITICAL_VULNS"
          
          if [ "$CRITICAL_VULNS" -gt 0 ]; then
            echo "‚ùå Critical vulnerabilities found!"
            exit 1
          elif [ "$HIGH_VULNS" -gt 0 ]; then
            echo "‚ö†Ô∏è High vulnerabilities found!"
            # Don't fail the build for high vulns, but warn
          fi

      - name: Check for secrets
        run: |
          # Simple check for potential secrets in code
          echo "Checking for potential secrets..."
          
          # Check for potential API keys, tokens, etc.
          SECRET_PATTERNS=(
            "api[_-]?key"
            "secret[_-]?key"
            "access[_-]?token"
            "auth[_-]?token"
            "discord[_-]?token"
            "password"
          )
          
          for pattern in "${SECRET_PATTERNS[@]}"; do
            if git grep -i "$pattern" -- '*.js' '*.json' | grep -v test | grep -v mock | grep -v example; then
              echo "‚ö†Ô∏è Potential secret found: $pattern"
            fi
          done

      # Security tests don't generate source code coverage since they focus on input validation

      - name: Store security test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-test-results
          path: |
            security-audit.json
            security-report.json
            test-results/security/
          retention-days: 30

  # Aggregate results and report
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, security-tests]
    if: always() && needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install coverage tools
        run: |
          npm install -g nyc

      - name: Download Node 18 unit test coverage
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: unit-test-results-node18
          path: test-results/unit/node18/

      - name: Download Node 20 unit test coverage
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: unit-test-results-node20
          path: test-results/unit/node20/

      - name: Download integration test results
        uses: actions/download-artifact@v4
        with:
          name: integration-test-results
          path: test-results/integration/

      - name: Download e2e test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: e2e-test-results
          path: test-results/e2e/

      - name: Download performance test coverage
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: performance-test-results
          path: test-results/performance/

      - name: Download security test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: security-test-results
          path: test-results/security/

      - name: Merge coverage reports (avoid triple-counting)
        id: coverage
        run: |
          mkdir -p coverage/merged
          
          # Find coverage files from test types that generate coverage (unit, integration, performance)
          echo "Collecting coverage from test types that generate source code coverage..."
          
          # First, ensure we have proper directory structure
          ls -laR test-results/ || echo "No test-results directory found"
          
          # Find all lcov.info files but prioritize Node 18 over Node 20 to avoid duplicates
          COVERAGE_FILES=()
          
          # Add Node 18 unit coverage (primary)
          if [ -f "test-results/unit/node18/coverage/unit/lcov.info" ]; then
            COVERAGE_FILES+=("test-results/unit/node18/coverage/unit/lcov.info")
            echo "Found Node 18 unit coverage"
          fi
          
          # Add integration coverage if available
          if [ -f "test-results/integration/coverage/integration/lcov.info" ]; then
            COVERAGE_FILES+=("test-results/integration/coverage/integration/lcov.info")
            echo "Found integration coverage"
          fi
          
          # Add performance coverage if available
          if [ -f "test-results/performance/coverage/performance/lcov.info" ]; then
            COVERAGE_FILES+=("test-results/performance/coverage/performance/lcov.info")
            echo "Found performance coverage"
          fi
          
          if [ ${#COVERAGE_FILES[@]} -gt 0 ]; then
            echo "Merging the following coverage files:"
            printf '%s\n' "${COVERAGE_FILES[@]}"
            
            # Use nyc to merge coverage reports
            mkdir -p .nyc_output
            
            # Copy all lcov files into a temporary directory for nyc to find
            for file in "${COVERAGE_FILES[@]}"; do
              cp "$file" "coverage/merged/$(basename "$file" .info)-$(uuidgen).info"
            done
            
            # Run the merge
            npx nyc merge coverage/merged .nyc_output/coverage.json
            
            # Generate the final lcov report from the merged data
            npx nyc report --reporter=lcov --report-dir=coverage/merged --temp-dir=.nyc_output
            
            if [ -f coverage/merged/lcov.info ]; then
              echo "Successfully merged coverage reports"
              
              # Debug: Check what's in the merged coverage
              echo "Merged coverage file size: $(wc -l < coverage/merged/lcov.info) lines"
              echo "Sample of merged coverage:"
              head -20 coverage/merged/lcov.info
              echo ""
              echo "NYC temp directory contents:"
              ls -la .nyc_output/
              echo "Sample of coverage.json:"
              head -10 .nyc_output/coverage.json
              
              # Generate summary from merged coverage - try with and without include filter
              npx nyc report --reporter=json-summary --temp-dir=.nyc_output > coverage-summary.json
              echo "Generated coverage-summary.json:"
              cat coverage-summary.json
              
              # Check if coverage summary was generated successfully
              if [ -s coverage-summary.json ] && [ "$(jq -r '.total.lines.total // 0' coverage-summary.json)" != "0" ]; then
                LINES_PCT=$(jq -r '.total.lines.pct // 0' coverage-summary.json)
                echo "Successfully generated coverage summary with ${LINES_PCT}% coverage"
                echo "coverage_pct=${LINES_PCT}" >> $GITHUB_OUTPUT
                echo "status=available" >> $GITHUB_OUTPUT
              else
                echo "Coverage summary is empty, trying with source file filter..."
                npx nyc report --reporter=json-summary --temp-dir=.nyc_output --include='src/**/*.js' --include='*.js' > coverage-summary.json
                
                LINES_PCT=$(jq -r '.total.lines.pct // 0' coverage-summary.json 2>/dev/null || echo "0")
                echo "Generated coverage with filter: ${LINES_PCT}% coverage"
                echo "coverage_pct=${LINES_PCT}" >> $GITHUB_OUTPUT
                echo "status=available" >> $GITHUB_OUTPUT
              fi
            else
              echo "Failed to merge coverage reports"
              echo '{"total":{"lines":{"pct":0},"statements":{"pct":0},"functions":{"pct":0},"branches":{"pct":0}}}' > coverage-summary.json
              echo "coverage_pct=0" >> $GITHUB_OUTPUT
              echo "status=missing" >> $GITHUB_OUTPUT
            fi
          else
            echo "No coverage files found to merge."
            echo '{"total":{"lines":{"pct":0},"statements":{"pct":0},"functions":{"pct":0},"branches":{"pct":0}}}' > coverage-summary.json
            echo "coverage_pct=0" >> $GITHUB_OUTPUT
            echo "status=missing" >> $GITHUB_OUTPUT
          fi

      - name: Generate comprehensive test report
        run: |
          mkdir -p reports
          OVERALL_COVERAGE=$(jq -r '.total.lines.pct // 0' coverage-summary.json 2>/dev/null | awk '{printf "%.2f", $1}' || echo "0.00")
          
          cat > reports/test-summary.md << EOF
          # Test Summary Report
          
          **Generated:** $(date)
          **Commit:** \`${{ github.sha }}\`
          **Branch:** \`${{ github.ref_name }}\`
          **Trigger:** ${{ github.event_name }}
          
          ## üéØ Overall Merged Coverage: ${OVERALL_COVERAGE}%
          *This is the accumulated coverage from all test suites that ran.*
          
          ---
          
          ## Test Suite Status
          
          | Test Type   | Result  |
          |-------------|---------|
          EOF
          
          # This loop now ONLY reports job status, not confusing individual coverage.
          for test_type in unit integration e2e performance security; do
            job_status=""
            
            case $test_type in
              "unit") job_status="${{ needs.unit-tests.result }}" ;;
              "integration") job_status="${{ needs.integration-tests.result }}" ;;
              "e2e") job_status="${{ needs.e2e-tests.result }}" ;;
              "performance") job_status="${{ needs.performance-tests.result }}" ;;
              "security") job_status="${{ needs.security-tests.result }}" ;;
            esac
            
            # Determine test status based on job result
            if [[ -n "$job_status" && "$job_status" != "skipped" ]]; then
              case "$job_status" in
                "success") test_status="‚úÖ Pass" ;;
                "failure") test_status="‚ùå Fail" ;;
                "cancelled") test_status="üõë Cancel" ;;
                *) test_status="‚ö†Ô∏è Unknown" ;;
              esac
              printf "| %-11s | %-7s |\n" "${test_type^}" "$test_status" >> reports/test-summary.md
            fi
          done
          
          echo "" >> reports/test-summary.md
          echo "## Detailed Results" >> reports/test-summary.md
          
          for test_type in unit integration e2e performance security; do
            echo "" >> reports/test-summary.md
            echo "### ${test_type^} Tests" >> reports/test-summary.md
            echo "" >> reports/test-summary.md
            echo "\`\`\`" >> reports/test-summary.md
            if [ "$test_type" == "unit" ]; then
              if [ -f test-results/unit/node18/coverage/unit/test-output-node18.log ]; then
                cat test-results/unit/node18/coverage/unit/test-output-node18.log >> reports/test-summary.md
              else
                echo "No output for Node 18" >> reports/test-summary.md
              fi
              echo "" >> reports/test-summary.md
              echo "---" >> reports/test-summary.md
              echo "" >> reports/test-summary.md
              if [ -f test-results/unit/node20/coverage/unit/test-output-node20.log ]; then
                cat test-results/unit/node20/coverage/unit/test-output-node20.log >> reports/test-summary.md
              else
                echo "No output for Node 20" >> reports/test-summary.md
              fi
            else
              if [ -f test-results/$test_type/coverage/$test_type/test-output.log ]; then
                 cat test-results/$test_type/coverage/$test_type/test-output.log >> reports/test-summary.md
              elif [ -f test-results/$test_type/test-output.log ]; then
                cat test-results/$test_type/test-output.log >> reports/test-summary.md
              else
                echo "No output for $test_type tests" >> reports/test-summary.md
              fi
            fi
            echo "\`\`\`" >> reports/test-summary.md
          done
          
          echo "" >> reports/test-summary.md
          echo "Detailed test results and coverage reports are available in the individual test artifacts:" >> reports/test-summary.md
          echo "- \`unit-test-results-node18\`, \`unit-test-results-node20\`" >> reports/test-summary.md
          echo "- \`integration-test-results\`" >> reports/test-summary.md
          echo "- \`e2e-test-results\`" >> reports/test-summary.md
          echo "- \`performance-test-results\`" >> reports/test-summary.md
          echo "- \`security-test-results\`" >> reports/test-summary.md
          
          # Add overall coverage summary
          if [ -f coverage-summary.json ]; then
            echo "" >> reports/test-summary.md
            echo "## Merged Coverage Breakdown" >> reports/test-summary.md
            echo "" >> reports/test-summary.md
            
            BRANCH_COVERAGE=$(jq -r '.total.branches.pct // 0' coverage-summary.json 2>/dev/null | awk '{printf "%.2f", $1}' || echo "0.00")
            FUNCTION_COVERAGE=$(jq -r '.total.functions.pct // 0' coverage-summary.json 2>/dev/null | awk '{printf "%.2f", $1}' || echo "0.00")
            
            echo "- **Lines:** ${OVERALL_COVERAGE}%" >> reports/test-summary.md
            echo "- **Branches:** ${BRANCH_COVERAGE}%" >> reports/test-summary.md  
            echo "- **Functions:** ${FUNCTION_COVERAGE}%" >> reports/test-summary.md
            echo "" >> reports/test-summary.md
            
            # Coverage quality assessment using integer comparison
            COVERAGE_INT=$(awk -v val="$OVERALL_COVERAGE" 'BEGIN {printf "%.0f", val * 100}')
            if [ "$COVERAGE_INT" -ge 2500 ]; then
              echo "‚úÖ **Coverage Quality:** Meets minimum standards (‚â•25%)" >> reports/test-summary.md
            elif [ "$COVERAGE_INT" -ge 1500 ]; then
              echo "‚ö†Ô∏è **Coverage Quality:** Below target but acceptable (‚â•15%)" >> reports/test-summary.md
            else
              echo "‚ùå **Coverage Quality:** Below minimum standards (<15%)" >> reports/test-summary.md
            fi
          fi

      - name: Validate coverage quality
        run: |
          if [ "${{ steps.coverage.outputs.status }}" == "available" ]; then
            COVERAGE="${{ steps.coverage.outputs.coverage_pct }}"
            # Ensure COVERAGE is a valid number
            COVERAGE=$(echo "$COVERAGE" | awk '{if($1 == "") print "0"; else printf "%.2f", $1}')
            
            echo "Current coverage: ${COVERAGE}%"
            
            # Set coverage quality gates using integer comparison
            COVERAGE_INT=$(awk -v val="$COVERAGE" 'BEGIN {printf "%.0f", val * 100}')
            if [ "$COVERAGE_INT" -lt 1000 ]; then
              echo "‚ùå Coverage critically low: $COVERAGE%"
              echo "coverage_status=critical" >> $GITHUB_ENV
            elif [ "$COVERAGE_INT" -lt 1500 ]; then
              echo "‚ö†Ô∏è Coverage below minimum: $COVERAGE%"
              echo "coverage_status=warning" >> $GITHUB_ENV
            elif [ "$COVERAGE_INT" -ge 2500 ]; then
              echo "‚úÖ Coverage meets target: $COVERAGE%"
              echo "coverage_status=good" >> $GITHUB_ENV
            else
              echo "üìà Coverage progressing: $COVERAGE%"
              echo "coverage_status=progress" >> $GITHUB_ENV
            fi
            
            # Create coverage metrics for tracking - Ensure all values are valid numbers
            BRANCHES_PCT=$(jq -r '.total.branches.pct // 0' coverage-summary.json 2>/dev/null | awk '{if($1 == "") print "0"; else printf "%.2f", $1}')
            FUNCTIONS_PCT=$(jq -r '.total.functions.pct // 0' coverage-summary.json 2>/dev/null | awk '{if($1 == "") print "0"; else printf "%.2f", $1}')
            STATEMENTS_PCT=$(jq -r '.total.statements.pct // 0' coverage-summary.json 2>/dev/null | awk '{if($1 == "") print "0"; else printf "%.2f", $1}')
            QUALITY_SCORE=$(awk -v cov="$COVERAGE" -v br="$BRANCHES_PCT" -v fn="$FUNCTIONS_PCT" 'BEGIN {printf "%.2f", cov * 0.4 + br * 0.3 + fn * 0.3}')
            
            echo "{
              \"timestamp\": \"$(date -Iseconds)\",
              \"commit\": \"${{ github.sha }}\",
              \"branch\": \"${{ github.ref_name }}\",
              \"coverage\": {
                \"lines\": $COVERAGE,
                \"branches\": $BRANCHES_PCT,
                \"functions\": $FUNCTIONS_PCT,
                \"statements\": $STATEMENTS_PCT
              },
              \"quality_score\": $QUALITY_SCORE
            }" > coverage-metrics.json
          else
            echo "No coverage data available"
            echo "coverage_status=missing" >> $GITHUB_ENV
          fi

      - name: Upload merged coverage to Codecov
        if: steps.coverage.outputs.status == 'available'
        uses: codecov/codecov-action@v4
        with:
          file: coverage/merged/lcov.info
          flags: merged
          name: merged-coverage
          fail_ci_if_error: false
          plugins: ''

      - name: Comment commit with test results
        if: github.event_name == 'push'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            const path = 'reports/test-summary.md';
            
            if (fs.existsSync(path)) {
              try {
                const body = fs.readFileSync(path, 'utf8');
                
                await github.rest.repos.createCommitComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  commit_sha: context.sha,
                  body: body
                });
                console.log('Commit comment created successfully');
              } catch (error) {
                console.log('Failed to create commit comment:', error.message);
              }
            } else {
              console.log('Test summary file not found');
            }

      - name: Store final test report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: |
            reports/
            coverage/merged/
            coverage-summary.json
            coverage-metrics.json
            test-results/
          retention-days: 30

      - name: Update status check
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "‚úÖ All tests completed successfully"
          else
            echo "‚ùå Some tests failed or were skipped"
            exit 1
          fi

# Cleanup job to remove old artifacts
  cleanup:
    name: Cleanup Old Artifacts
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
      - name: Delete old artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const cutoff = new Date();
            cutoff.setDate(cutoff.getDate() - 30); // Keep artifacts for 30 days
            
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
            });
            
            for (const artifact of artifacts.data.artifacts) {
              if (new Date(artifact.created_at) < cutoff) {
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id,
                });
                console.log(`Deleted artifact: ${artifact.name}`);
              }
            }